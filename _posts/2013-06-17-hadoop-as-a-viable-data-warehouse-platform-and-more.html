---
layout: post
title: Hadoop as a Viable Data Warehouse Platform, and More
tags: [Big Data]
---

<br /><div class="p1"><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-LCKA8w8ZtOQ/Ub9BsDFNcpI/AAAAAAAAAKc/gAjm9xlSrzo/s1600/hadoop-logo.jpg" imageanchor="1" style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;"><img border="0" src="http://4.bp.blogspot.com/-LCKA8w8ZtOQ/Ub9BsDFNcpI/AAAAAAAAAKc/gAjm9xlSrzo/s1600/hadoop-logo.jpg" /></a></div><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">In 2011 we made an important decision to take a barely worn path and initiate a new project to replace our core data warehouse (DWH) platform with a <a href="http://hadoop.apache.org/">Hadoop</a>-based implementation. &nbsp;Like many organizations we were experiencing scalability, complexity and cost issues with our traditional DWH model, and new development time and big data growth were creating a significant resource drain on the company.</span><br /><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">With no closely comparable reference implementations or internal Hadoop experience to speak of, we charged a small team of java developers to lead this effort with the goal of leveraging Hadoop to expand the scope of our DWH to become the system of record not just for our unstructured data (clickstream and ad impressions) but also structured data like inventory and vehicle specification sets.</span><br /><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">The first several months of this initiative were pretty slow going, with the team learning the ins and outs of </span><a href="http://hadoop.apache.org/docs/stable/hdfs_design.html" style="font-family: Arial, Helvetica, sans-serif;">HDFS</a><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">, </span><a href="http://hadoop.apache.org/docs/stable/mapred_tutorial.html#Purpose" style="font-family: Arial, Helvetica, sans-serif;">MapReduce</a><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">, </span><a href="http://hbase.apache.org/" style="font-family: Arial, Helvetica, sans-serif;">HBase</a><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">, </span><a href="http://oozie.apache.org/" style="font-family: Arial, Helvetica, sans-serif;">Oozie</a><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">, </span><a href="http://hive.apache.org/" style="font-family: Arial, Helvetica, sans-serif;">Hive</a><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">, and </span><a href="http://pig.apache.org/" style="font-family: Arial, Helvetica, sans-serif;">Pig</a><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">. &nbsp;However, once the team had leveraged the framework to ingest, process and version our existing DWH data, we brought in the Analytics and BI teams to begin porting our reporting and ad hoc query infrastructure to leverage Hadoop. &nbsp;And in February of this year we actually turned off the </span><a href="http://en.wikipedia.org/wiki/Extract,_transform,_load" style="font-family: Arial, Helvetica, sans-serif;">ETL</a><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"> jobs for our legacy DWH, with all Edmunds.com reporting functions now operating from Hadoop-managed data.</span><br /><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Was it a success? &nbsp;Is there an ROI? &nbsp;What's the best way to get started leveraging Hadoop? Turning off our legacy ETL was the literal definition of project success, but it has exceeded our expectations by becoming a central repository for Edmunds data rather than just a reporting endpoint. &nbsp;One effort alone answers the ROI question, and since February the Business Analysts have used the combined data sets and increased processing capabilities to save over $1.7M from our paid search marketing budget through keyword bidding optimization.</span><br /><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">For more Hadoop-specific questions, I sat down with Greg Rokita, Edmunds.com's Sr. Director of Software Architecture and Hadoop team lead to get his take on the project and how Hadoop and HBase will keep the Technology and Analytics teams ahead of Edmunds.com's Big Data demands.</span></div><div class="p2"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">----</span></div><div class="p2"><b><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Phil: &nbsp;Can you first provide a brief description of Hadoop and HBase for those that might not be familiar with this framework?</span></b></div><div class="p2"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p3"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Greg: &nbsp;Hadoop is composed of two subsystems:&nbsp;Hadoop&nbsp;Distributed File System that&nbsp;provides scalable, reliable and inexpensive storage&nbsp;and the MapReduce processing framework that allows for processing large quantities of data in a distributed and reliable manner.&nbsp;If you consider a database to be an abstraction on top&nbsp;of a filesystem then, by analogy, HBase is an abstraction on top of&nbsp;HDFS that provides users with higher level functionality rather than just files. &nbsp;</span></div><div class="p4"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p4"><b><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Phil: &nbsp;What made Hadoop a good call for our next generation DWH platform? &nbsp;What were your concerns at the time?</span></b><br /><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Greg: &nbsp; We had always felt a need for a better DWH that processed all&nbsp;relevant data in a timely manner with limited failures. We wanted to tackle computationally&nbsp;and data intensive projects like paid search marketing optimization that just couldn't be run on our existing platform. Hadoop provided a cost-effective, industry standard approach to fulfill our growing needs. Our concerns were mostly related to the maturity of the platform, but with time that concern was alleviated&nbsp;as Hadoop distributions became more robust and broadly supported.</span><br /><b><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></b><b><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Phil: &nbsp;What are the primary use cases for HBase in our environment?</span></b><br /><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Greg: &nbsp;We use HBase to store and version&nbsp;all of the structured data processed by DWH including inventory, transactions, leads, dealer information and vehicle configuration. Structured data is correlated with session events (clickstream) and the resulting aggregates are stored in HBase. The aggregated data is eventually published to web applications, reporting systems (Microstrategy, Platfora) and ad-hoc query systems (Netezza, Redshift) .&nbsp;</span></div><div class="p4"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p1"><b><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Phil: &nbsp;What does the column-oriented structure of HBase offer that wasn't easily accomplished with out traditional RDBMS?</span></b></div><div class="p2"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p3"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Greg: &nbsp;HBase's flexible, column-oriented structure allows us to organize all events for a particular session/visitor in a single row.&nbsp;&nbsp;Among other benefits, such organization provides for an elegant and performant solution&nbsp;to the session carry-over problem. The fixed relational schema structure of the legacy DWH&nbsp;placed&nbsp;artificial&nbsp;limits on the session length and induced overly complex processing logic.&nbsp;</span></div><div class="p4"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p4"><b><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Phil: &nbsp;How effective is HBase at dealing with different data types? How is versioning used?</span></b></div><div class="p2"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p3"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Greg: &nbsp;HBase treats all data as binary and leaves up to the user the choice of data serialization. This flexibility allows us to not only store simple data types but also complex objects represented by Thrift.&nbsp;&nbsp;HBase's built-in versioning allows us to recreate and republish all historical Edmunds datasets. For example, we can determine what was the lot inventory of a particular dealer 2 weeks ago, even though our inventory source systems only keep track of the current state of inventory.</span></div><div class="p4"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p4"><b><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Phil: &nbsp;Describe the main differences between our ETL process pre and post Hadoop</span></b><br /><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Greg: &nbsp;Legacy DWH ETL processes were implemented with Informatica workflows that operated </span><a href="http://en.wikipedia.org/wiki/RDBMS" style="font-family: Arial, Helvetica, sans-serif;">Relational Database Management Systems (RDBMS)</a><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">.&nbsp;&nbsp;Aside from RDBMS intrinsic&nbsp;scalability and cost&nbsp;issues, the database was&nbsp;concurrently&nbsp;used by the users and reporting tools&nbsp;compounding&nbsp;the&nbsp;performance&nbsp;problems. The new DWH separates data processing (Hadoop and HBase) from user access and reporting (</span><a href="http://www-01.ibm.com/software/data/netezza/" style="font-family: Arial, Helvetica, sans-serif;">Netezza</a><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"> and </span><a href="http://aws.amazon.com/redshift/" style="font-family: Arial, Helvetica, sans-serif;">Redshift</a><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">). Scalability of the new system allows us to&nbsp;quickly&nbsp;and easily&nbsp;reprocess&nbsp;and test workflows. The processing speed combined with architectural and design&nbsp;choices we made reduced the DWH project development cycle for new requirements from months to weeks. Moreover, the&nbsp;scalability&nbsp;of the new platform allowed us to complete projects like paid search marketing optimization that was attempted&nbsp;several times on the legacy&nbsp;platform&nbsp;without success. &nbsp;Actual data ingestion times have been reduced from 12+ hours each day to less than an hour.</span></div><div class="p4"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p1"><b><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Phil: &nbsp;Do you see any limitations in this new architecture with your current line of sight into big data challenges?</span></b></div><div class="p4"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p3"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Greg: &nbsp;One limitation of the platform is its batch oriented nature.&nbsp;&nbsp;However, the new release of Hadoop--YARN allows other computational&nbsp;paradigms&nbsp;aside from Map-Reduce. Tools and frameworks such as&nbsp;Impala, Cloudera Search and DataTorrent&nbsp;&nbsp;add powerful real-time capabilities to the platform.</span></div><div class="p2"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p2"><b><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Phil: &nbsp;What were the key decisions that made this project a success?</span></b><br /><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Greg: &nbsp;We tried to free ourselves from DWH dogmas and approached the problem with a regular software project focus on performance,&nbsp;reliability&nbsp;and reducing complexity knowing that we have to handle a large&nbsp;quantity of&nbsp;data and&nbsp;numerous data sets.&nbsp;We made a&nbsp;conscious&nbsp;decision to eliminate&nbsp;dependencies&nbsp;on outside resources. For example, we use hash functions rather than&nbsp;stateful systems for ID generation.&nbsp;&nbsp;(sequence&nbsp;number&nbsp;generation&nbsp;was a major&nbsp;performance&nbsp;bottleneck for&nbsp;the legacy DWH). We also abstracted complex&nbsp;vehicle classification (make, model, submodel, style) with a model that simplifies how other data sets refer to vehicle configuration.&nbsp;</span><i style="color: #444444; font-family: Arial, Helvetica, sans-serif;">&nbsp;</i></div><div class="p2"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p1"><b><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Phil: &nbsp;How did your team become Hadoop experts? &nbsp;What was their background?</span></b></div><div class="p2"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Greg: &nbsp;The team had no prior Hadoop experience. Most of the team members had solid Java experience, but most&nbsp;importantly, everyone had a curiosity and willingness to learn new things.&nbsp;&nbsp;We strongly&nbsp;encourage continuous&nbsp;refactoring to constantly&nbsp;improve the&nbsp;scalability&nbsp;and&nbsp;reliability&nbsp;of our jobs. Most refactors lead to pattern discovery that we can incorporate across&nbsp;different&nbsp;workflows. For example, the Data Provider pattern abstracts common interactions with HBase, speeds up development, simplifies and standardizes the code and asserts optimal performance across the modules.</span></div><div class="p2"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p1"><b><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">What is your advice to others considering adopting or experimenting with Hadoop? How do you recommend getting your feet wet?</span></b></div><div class="p4"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p3"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Don't try to come up with perfect solution the first time around. Make all your operations idempotent and make sure that you have ability to easily rerun your workflows so that you can constantly refactor and improve. Avoid creating run-time dependencies on non-Hadoop based services. To get your feet wet, read up on basic Hadoop architecture and pick a use case that benefits from parallel execution. Once you can run the job reliably, try to optimize your code for the fastest performance with the minimum&nbsp;possible&nbsp;resource&nbsp;utilization.&nbsp;</span></div><div class="p2"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;"><br /></span></div><div class="p2"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Phil Potloff</span></div><div class="p1"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">EVP, CIO</span></div><div class="p1"><span style="color: #444444; font-family: Arial, Helvetica, sans-serif;">Edmunds.com</span></div>
